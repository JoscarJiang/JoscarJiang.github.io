<hr>
<p>title: 斯坦福博士李纪为：如何打造对话机器人 直播笔记（太长不看版）<br>
catalog: true<br>
tags:</p>
<ul>
<li>Deep Learning</li>
<li>Chatbot<br>
catagories:</li>
<li>Chatbot</li>
</ul>
<hr>
<p>李博士在直播中主要针对对话系统的4个难点进行了探讨：</p>
<ul>
<li>生成有意义的答复( Generating Interesting and Informative Responses ).</li>
<li>保持说话者的一致性( How to preserve Speaker Consistency ).</li>
<li>多轮对话系统 ( Long-term Conversational Success ).</li>
<li>让机器人学会提问( Giving bots the ability to ask questions ).</li>
</ul>
<h2>有意义的答复</h2>
<h4>Seq2Seq Model</h4>
<p>Seq2Seq模型最早用于神经机器翻译，也经常被用于对话系统，见下图。<br>
<img src="./1503124923775.png" alt="|center|500x300"><br>
论文地址</p>
<ol>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="http://www.aclweb.org/anthology/P15-1001">On Using Very Large Target Vocabulary for Neural Machine Translation</a></li>
<li><a href="http://aclweb.org/anthology/D15-1166">Effective Approaches to Attention-based Neural Machine Translation</a></li>
</ol>
<h4>问题</h4>
<p>Seq2Seq模型存在一些问题，比如：<strong>总是产生诸如”我不知道“等无意义的回答</strong>，因为此回答在训练集中非常常见。<br>
解决的第一种思路也许是通过规则或者其他手段不允许产生此类回答，但是类似”我不知道“的回答有很多，没办法消除干净。<br>
解决的第二种思路是可以利用<strong>互信息(Mutual Information)<strong>在训练的时候过滤没有意义的答复。一般来说，给出问题，我们可以给出一个相应的答案；反过来说，给出</strong>有意义的答案</strong>，我们也可以大致推得问题。所以，在损失函数中通过加入后验概率$\log p(message|responce)$即可。<br>
<img src="./1503127207851.png" alt="|center|500x220"><br>
<img src="./1503128210103.png" alt="|center|500x220"></p>
<p>论文：<a href="https://arxiv.org/pdf/1510.03055.pdf">A Diversity-Promoting Objective Function for Neural Conversation Models</a></p>
<h2>保持说话者的一致性</h2>
<p>因为训练集通常来自于不同的人，所以如果不加处理，即使对于同一个问题，不同的问法可能会得到前后不一致的回答，更别说多轮对话中可能出现的矛盾。所以，保持对话机器人身份等一致性非常重要。<br>
作者提出了persona-based models 用以处理这类问题——将人物信息比如背景、说话方式等抽取成向量表示，结合进Seq2Seq模型中。</p>
<p>论文:<a href="https://arxiv.org/pdf/1603.06155.pdf">A Persona-Based Neural Conversation Model</a></p>
<h2>多轮对话系统</h2>
<p>多轮对话系统设计非常难，主要有以下几个问题：</p>
<ul>
<li>重复回答（Repetitive responses）</li>
<li>短视的对话决定（Short-sighted conversation decisions）</li>
</ul>
<p><img src="./1503130517816.png" alt="|center|500x270"><br>
<img src="./1503130525017.png" alt="|center|500x270"></p>
<h3>解决方案：增强学习</h3>
<p>State：之前对话encoding的输出<br>
Action: 下一句对话<br>
<strong>Reward</strong>:<br>
主要基于三点</p>
<ul>
<li>对话的简易程度( Ease of answering )<br>
$$ r(response) = -log(dull \  utterances|response) $$</li>
<li>信息流( Information Flow ) ——考量回答的重复程度<br>
$$ r_2 = -log\ Sigmoid(cos(s_1,s_2)) $$</li>
<li>意义( Meaningfulness )<br>
$$ r = log\ p(response|message)+log\ p(message|response) $$</li>
</ul>
<p>论文：<a href="https://arxiv.org/pdf/1606.01541.pdf">Deep Reinforcement Learning for Dialogue Generation</a></p>
<p>但是Reward这些特征都是人为设定的，还有许多特征没能被考量。有一个思路是通过人为去判断答复是来自机器还是人（图灵测试），但是这样的代价太昂贵，但是可以考虑用<strong>GAN</strong>。<br>
<img src="./1503132113832.png" alt="|center|500x300"></p>
<p>论文：<a href="https://arxiv.org/pdf/1701.06547.pdf">Adversarial Learning for Neural Dialogue Generation</a></p>
<h2>让机器人学会提问</h2>
<p>三类需要提问的情况：</p>
<ol>
<li>要求说明解释(Ask for Text Clarification)：比如遇到复杂长句（机器人擅长处理清晰的短句）</li>
<li>要求提示( Ask for Hints )：比如线索不够需要更多的关键词去搜索</li>
<li>遇到新知识( Ask about New Knowledge )：比如不认识的人名、地名等</li>
</ol>
<p>怎么做：训练一个增强学习系统<br>
论文：<a href="http://stanford.edu/~jiweil/AQ.pdf">LEARNING THROUGH DIALOGUE INTERACTIONS</a></p>
<p>最后李博士还提到了一篇论文：<br>
<a href="https://arxiv.org/pdf/1705.10229.pdf">Latent Intention Dialogue Models</a><br>
提出了一个多轮对话的任务型系统模型LIDM</p>
